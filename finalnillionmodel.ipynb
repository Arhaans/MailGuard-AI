{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBsfjTHbzXsk",
        "outputId": "31779af5-f414-4f8b-d4d0-eee763b6a26b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.46.0-py3-none-any.whl.metadata (44 kB)\n",
            "     ---------------------------------------- 0.0/44.1 kB ? eta -:--:--\n",
            "     --------- ------------------------------ 10.2/44.1 kB ? eta -:--:--\n",
            "     ----------------------------------- -- 41.0/44.1 kB 393.8 kB/s eta 0:00:01\n",
            "     -------------------------------------- 44.1/44.1 kB 310.9 kB/s eta 0:00:00\n",
            "Collecting torch\n",
            "  Downloading torch-2.5.0-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.5.2-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
            "Collecting onnx\n",
            "  Downloading onnx-1.17.0-cp312-cp312-win_amd64.whl.metadata (16 kB)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.19.2-cp312-cp312-win_amd64.whl.metadata (4.7 kB)\n",
            "Collecting filelock (from transformers)\n",
            "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
            "  Downloading huggingface_hub-0.26.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\arhaa\\desktop\\lib\\site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\arhaa\\desktop\\lib\\site-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\arhaa\\desktop\\lib\\site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\arhaa\\desktop\\lib\\site-packages (from transformers) (2024.7.24)\n",
            "Requirement already satisfied: requests in c:\\users\\arhaa\\desktop\\lib\\site-packages (from transformers) (2.32.3)\n",
            "Collecting safetensors>=0.4.1 (from transformers)\n",
            "  Downloading safetensors-0.4.5-cp312-none-win_amd64.whl.metadata (3.9 kB)\n",
            "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
            "  Downloading tokenizers-0.20.1-cp312-none-win_amd64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\arhaa\\desktop\\lib\\site-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\arhaa\\desktop\\lib\\site-packages (from torch) (4.12.2)\n",
            "Collecting networkx (from torch)\n",
            "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\arhaa\\desktop\\lib\\site-packages (from torch) (3.1.4)\n",
            "Collecting fsspec (from torch)\n",
            "  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting setuptools (from torch)\n",
            "  Downloading setuptools-75.2.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting sympy==1.13.1 (from torch)\n",
            "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting scipy>=1.6.0 (from scikit-learn)\n",
            "  Downloading scipy-1.14.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
            "     ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
            "     ---------------------------------------- 60.8/60.8 kB ? eta 0:00:00\n",
            "Collecting joblib>=1.2.0 (from scikit-learn)\n",
            "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
            "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting protobuf>=3.20.2 (from onnx)\n",
            "  Downloading protobuf-5.28.3-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting flatbuffers (from onnxruntime)\n",
            "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
            "Requirement already satisfied: colorama in c:\\users\\arhaa\\desktop\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\arhaa\\desktop\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\arhaa\\desktop\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\arhaa\\desktop\\lib\\site-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\arhaa\\desktop\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arhaa\\desktop\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
            "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime)\n",
            "  Downloading pyreadline3-3.5.4-py3-none-any.whl.metadata (4.7 kB)\n",
            "Downloading transformers-4.46.0-py3-none-any.whl (10.0 MB)\n",
            "   ---------------------------------------- 0.0/10.0 MB ? eta -:--:--\n",
            "   ----- ---------------------------------- 1.3/10.0 MB 41.6 MB/s eta 0:00:01\n",
            "   ---------- ----------------------------- 2.5/10.0 MB 32.6 MB/s eta 0:00:01\n",
            "   --------------- ------------------------ 3.9/10.0 MB 30.7 MB/s eta 0:00:01\n",
            "   ---------------------- ----------------- 5.6/10.0 MB 32.4 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 7.0/10.0 MB 32.2 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 8.8/10.0 MB 33.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------  10.0/10.0 MB 32.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 10.0/10.0 MB 30.6 MB/s eta 0:00:00\n",
            "Downloading torch-2.5.0-cp312-cp312-win_amd64.whl (203.1 MB)\n",
            "   ---------------------------------------- 0.0/203.1 MB ? eta -:--:--\n",
            "   ---------------------------------------- 1.2/203.1 MB 40.0 MB/s eta 0:00:06\n",
            "   ---------------------------------------- 2.3/203.1 MB 29.1 MB/s eta 0:00:07\n",
            "    --------------------------------------- 4.1/203.1 MB 32.8 MB/s eta 0:00:07\n",
            "   - -------------------------------------- 5.8/203.1 MB 33.9 MB/s eta 0:00:06\n",
            "   - -------------------------------------- 7.2/203.1 MB 32.8 MB/s eta 0:00:06\n",
            "   - -------------------------------------- 9.1/203.1 MB 34.3 MB/s eta 0:00:06\n",
            "   -- ------------------------------------- 10.8/203.1 MB 32.8 MB/s eta 0:00:06\n",
            "   -- ------------------------------------- 12.3/203.1 MB 36.4 MB/s eta 0:00:06\n",
            "   -- ------------------------------------- 13.5/203.1 MB 36.3 MB/s eta 0:00:06\n",
            "   -- ------------------------------------- 15.1/203.1 MB 32.8 MB/s eta 0:00:06\n",
            "   --- ------------------------------------ 16.7/203.1 MB 34.4 MB/s eta 0:00:06\n",
            "   --- ------------------------------------ 18.4/203.1 MB 34.4 MB/s eta 0:00:06\n",
            "   --- ------------------------------------ 20.2/203.1 MB 32.7 MB/s eta 0:00:06\n",
            "   ---- ----------------------------------- 22.0/203.1 MB 32.7 MB/s eta 0:00:06\n",
            "   ---- ----------------------------------- 23.6/203.1 MB 34.6 MB/s eta 0:00:06\n",
            "   ---- ----------------------------------- 24.7/203.1 MB 34.4 MB/s eta 0:00:06\n",
            "   ----- ---------------------------------- 26.9/203.1 MB 36.3 MB/s eta 0:00:05\n",
            "   ----- ---------------------------------- 28.1/203.1 MB 34.6 MB/s eta 0:00:06\n",
            "   ----- ---------------------------------- 30.0/203.1 MB 36.4 MB/s eta 0:00:05\n",
            "   ------ --------------------------------- 31.5/203.1 MB 36.4 MB/s eta 0:00:05\n",
            "   ------ --------------------------------- 32.6/203.1 MB 32.8 MB/s eta 0:00:06\n",
            "   ------ --------------------------------- 34.1/203.1 MB 34.4 MB/s eta 0:00:05\n",
            "   ------- -------------------------------- 36.0/203.1 MB 34.4 MB/s eta 0:00:05\n",
            "   ------- -------------------------------- 37.6/203.1 MB 34.4 MB/s eta 0:00:05\n",
            "   ------- -------------------------------- 39.2/203.1 MB 34.4 MB/s eta 0:00:05\n",
            "   -------- ------------------------------- 40.6/203.1 MB 32.8 MB/s eta 0:00:05\n",
            "   -------- ------------------------------- 41.4/203.1 MB 31.2 MB/s eta 0:00:06\n",
            "   -------- ------------------------------- 42.9/203.1 MB 32.8 MB/s eta 0:00:05\n",
            "   -------- ------------------------------- 44.0/203.1 MB 31.2 MB/s eta 0:00:06\n",
            "   --------- ------------------------------ 46.1/203.1 MB 31.2 MB/s eta 0:00:06\n",
            "   --------- ------------------------------ 47.5/203.1 MB 31.2 MB/s eta 0:00:05\n",
            "   --------- ------------------------------ 49.0/203.1 MB 31.2 MB/s eta 0:00:05\n",
            "   --------- ------------------------------ 50.7/203.1 MB 31.1 MB/s eta 0:00:05\n",
            "   ---------- ----------------------------- 52.2/203.1 MB 32.8 MB/s eta 0:00:05\n",
            "   ---------- ----------------------------- 53.9/203.1 MB 34.4 MB/s eta 0:00:05\n",
            "   ---------- ----------------------------- 55.5/203.1 MB 34.4 MB/s eta 0:00:05\n",
            "   ----------- ---------------------------- 56.9/203.1 MB 34.6 MB/s eta 0:00:05\n",
            "   ----------- ---------------------------- 58.9/203.1 MB 34.4 MB/s eta 0:00:05\n",
            "   ----------- ---------------------------- 60.1/203.1 MB 34.4 MB/s eta 0:00:05\n",
            "   ----------- ---------------------------- 60.4/203.1 MB 31.2 MB/s eta 0:00:05\n",
            "   ------------ --------------------------- 61.8/203.1 MB 31.2 MB/s eta 0:00:05\n",
            "   ------------ --------------------------- 62.7/203.1 MB 28.4 MB/s eta 0:00:05\n",
            "   ------------ --------------------------- 64.4/203.1 MB 31.2 MB/s eta 0:00:05\n",
            "   ------------- -------------------------- 66.4/203.1 MB 29.7 MB/s eta 0:00:05\n",
            "   ------------- -------------------------- 68.2/203.1 MB 29.8 MB/s eta 0:00:05\n",
            "   ------------- -------------------------- 69.8/203.1 MB 29.8 MB/s eta 0:00:05\n",
            "   -------------- ------------------------- 71.6/203.1 MB 36.4 MB/s eta 0:00:04\n",
            "   -------------- ------------------------- 73.1/203.1 MB 38.5 MB/s eta 0:00:04\n",
            "   -------------- ------------------------- 74.9/203.1 MB 38.5 MB/s eta 0:00:04\n",
            "   --------------- ------------------------ 76.4/203.1 MB 36.4 MB/s eta 0:00:04\n",
            "   --------------- ------------------------ 78.1/203.1 MB 36.3 MB/s eta 0:00:04\n",
            "   --------------- ------------------------ 79.7/203.1 MB 34.4 MB/s eta 0:00:04\n",
            "   ---------------- ----------------------- 81.4/203.1 MB 36.4 MB/s eta 0:00:04\n",
            "   ---------------- ----------------------- 83.1/203.1 MB 36.4 MB/s eta 0:00:04\n",
            "   ---------------- ----------------------- 84.7/203.1 MB 34.4 MB/s eta 0:00:04\n",
            "   ----------------- ---------------------- 86.3/203.1 MB 36.4 MB/s eta 0:00:04\n",
            "   ----------------- ---------------------- 88.1/203.1 MB 36.4 MB/s eta 0:00:04\n",
            "   ----------------- ---------------------- 89.5/203.1 MB 34.4 MB/s eta 0:00:04\n",
            "   ------------------ --------------------- 91.5/203.1 MB 36.3 MB/s eta 0:00:04\n",
            "   ------------------ --------------------- 92.9/203.1 MB 36.4 MB/s eta 0:00:04\n",
            "   ------------------ --------------------- 93.0/203.1 MB 31.2 MB/s eta 0:00:04\n",
            "   ------------------ --------------------- 93.9/203.1 MB 29.7 MB/s eta 0:00:04\n",
            "   ------------------ --------------------- 95.5/203.1 MB 29.7 MB/s eta 0:00:04\n",
            "   ------------------- -------------------- 97.2/203.1 MB 28.5 MB/s eta 0:00:04\n",
            "   ------------------- -------------------- 98.8/203.1 MB 28.5 MB/s eta 0:00:04\n",
            "   ------------------- ------------------- 100.4/203.1 MB 28.5 MB/s eta 0:00:04\n",
            "   ------------------- ------------------- 102.0/203.1 MB 28.5 MB/s eta 0:00:04\n",
            "   ------------------- ------------------- 103.6/203.1 MB 34.4 MB/s eta 0:00:03\n",
            "   -------------------- ------------------ 105.1/203.1 MB 34.4 MB/s eta 0:00:03\n",
            "   -------------------- ------------------ 106.8/203.1 MB 34.4 MB/s eta 0:00:03\n",
            "   -------------------- ------------------ 108.4/203.1 MB 34.4 MB/s eta 0:00:03\n",
            "   --------------------- ----------------- 110.0/203.1 MB 34.6 MB/s eta 0:00:03\n",
            "   --------------------- ----------------- 111.4/203.1 MB 34.4 MB/s eta 0:00:03\n",
            "   --------------------- ----------------- 113.1/203.1 MB 34.4 MB/s eta 0:00:03\n",
            "   --------------------- ----------------- 114.5/203.1 MB 34.4 MB/s eta 0:00:03\n",
            "   ---------------------- ---------------- 116.3/203.1 MB 34.4 MB/s eta 0:00:03\n",
            "   ---------------------- ---------------- 118.0/203.1 MB 34.4 MB/s eta 0:00:03\n",
            "   ---------------------- ---------------- 119.6/203.1 MB 34.4 MB/s eta 0:00:03\n",
            "   ----------------------- --------------- 121.0/203.1 MB 32.7 MB/s eta 0:00:03\n",
            "   ----------------------- --------------- 122.4/203.1 MB 32.8 MB/s eta 0:00:03\n",
            "   ----------------------- --------------- 124.1/203.1 MB 34.4 MB/s eta 0:00:03\n",
            "   ------------------------ -------------- 125.6/203.1 MB 32.8 MB/s eta 0:00:03\n",
            "   ------------------------ -------------- 127.2/203.1 MB 32.8 MB/s eta 0:00:03\n",
            "   ------------------------ -------------- 128.5/203.1 MB 32.7 MB/s eta 0:00:03\n",
            "   ------------------------ -------------- 129.8/203.1 MB 32.8 MB/s eta 0:00:03\n",
            "   ------------------------- ------------- 131.5/203.1 MB 32.8 MB/s eta 0:00:03\n",
            "   ------------------------- ------------- 133.1/203.1 MB 32.7 MB/s eta 0:00:03\n",
            "   ------------------------- ------------- 134.8/203.1 MB 34.4 MB/s eta 0:00:02\n",
            "   -------------------------- ------------ 136.5/203.1 MB 34.4 MB/s eta 0:00:02\n",
            "   -------------------------- ------------ 138.1/203.1 MB 34.4 MB/s eta 0:00:02\n",
            "   -------------------------- ------------ 139.7/203.1 MB 36.4 MB/s eta 0:00:02\n",
            "   --------------------------- ----------- 141.1/203.1 MB 34.4 MB/s eta 0:00:02\n",
            "   --------------------------- ----------- 143.0/203.1 MB 36.4 MB/s eta 0:00:02\n",
            "   --------------------------- ----------- 144.6/203.1 MB 36.4 MB/s eta 0:00:02\n",
            "   ---------------------------- ---------- 145.8/203.1 MB 34.4 MB/s eta 0:00:02\n",
            "   ---------------------------- ---------- 147.6/203.1 MB 34.4 MB/s eta 0:00:02\n",
            "   ---------------------------- ---------- 148.8/203.1 MB 32.7 MB/s eta 0:00:02\n",
            "   ---------------------------- ---------- 150.4/203.1 MB 32.7 MB/s eta 0:00:02\n",
            "   ----------------------------- --------- 151.8/203.1 MB 32.8 MB/s eta 0:00:02\n",
            "   ----------------------------- --------- 153.5/203.1 MB 32.8 MB/s eta 0:00:02\n",
            "   ----------------------------- --------- 154.6/203.1 MB 29.7 MB/s eta 0:00:02\n",
            "   ----------------------------- --------- 156.2/203.1 MB 31.2 MB/s eta 0:00:02\n",
            "   ------------------------------ -------- 157.7/203.1 MB 31.2 MB/s eta 0:00:02\n",
            "   ------------------------------ -------- 158.7/203.1 MB 29.7 MB/s eta 0:00:02\n",
            "   ------------------------------ -------- 160.5/203.1 MB 32.8 MB/s eta 0:00:02\n",
            "   ------------------------------- ------- 161.8/203.1 MB 31.2 MB/s eta 0:00:02\n",
            "   ------------------------------- ------- 164.0/203.1 MB 32.7 MB/s eta 0:00:02\n",
            "   ------------------------------- ------- 165.7/203.1 MB 34.4 MB/s eta 0:00:02\n",
            "   -------------------------------- ------ 167.1/203.1 MB 34.4 MB/s eta 0:00:02\n",
            "   -------------------------------- ------ 168.8/203.1 MB 36.4 MB/s eta 0:00:01\n",
            "   -------------------------------- ------ 170.4/203.1 MB 34.4 MB/s eta 0:00:01\n",
            "   --------------------------------- ----- 172.0/203.1 MB 36.4 MB/s eta 0:00:01\n",
            "   --------------------------------- ----- 173.7/203.1 MB 34.4 MB/s eta 0:00:01\n",
            "   --------------------------------- ----- 175.4/203.1 MB 34.4 MB/s eta 0:00:01\n",
            "   ---------------------------------- ---- 177.1/203.1 MB 36.4 MB/s eta 0:00:01\n",
            "   ---------------------------------- ---- 178.8/203.1 MB 36.3 MB/s eta 0:00:01\n",
            "   ---------------------------------- ---- 180.5/203.1 MB 36.3 MB/s eta 0:00:01\n",
            "   ----------------------------------- --- 182.2/203.1 MB 36.4 MB/s eta 0:00:01\n",
            "   ----------------------------------- --- 184.0/203.1 MB 36.4 MB/s eta 0:00:01\n",
            "   ----------------------------------- --- 185.8/203.1 MB 36.4 MB/s eta 0:00:01\n",
            "   ------------------------------------ -- 187.5/203.1 MB 36.4 MB/s eta 0:00:01\n",
            "   ------------------------------------ -- 189.3/203.1 MB 38.6 MB/s eta 0:00:01\n",
            "   ------------------------------------ -- 191.1/203.1 MB 36.4 MB/s eta 0:00:01\n",
            "   ------------------------------------ -- 192.5/203.1 MB 38.5 MB/s eta 0:00:01\n",
            "   ------------------------------------- - 193.9/203.1 MB 36.4 MB/s eta 0:00:01\n",
            "   ------------------------------------- - 195.3/203.1 MB 34.4 MB/s eta 0:00:01\n",
            "   ------------------------------------- - 197.5/203.1 MB 36.3 MB/s eta 0:00:01\n",
            "   --------------------------------------  199.0/203.1 MB 36.3 MB/s eta 0:00:01\n",
            "   --------------------------------------  200.8/203.1 MB 36.4 MB/s eta 0:00:01\n",
            "   --------------------------------------  202.5/203.1 MB 36.4 MB/s eta 0:00:01\n",
            "   --------------------------------------  203.1/203.1 MB 36.4 MB/s eta 0:00:01\n",
            "   --------------------------------------  203.1/203.1 MB 36.4 MB/s eta 0:00:01\n",
            "   --------------------------------------  203.1/203.1 MB 36.4 MB/s eta 0:00:01\n",
            "   --------------------------------------  203.1/203.1 MB 36.4 MB/s eta 0:00:01\n",
            "   --------------------------------------  203.1/203.1 MB 36.4 MB/s eta 0:00:01\n",
            "   --------------------------------------  203.1/203.1 MB 36.4 MB/s eta 0:00:01\n",
            "   --------------------------------------  203.1/203.1 MB 36.4 MB/s eta 0:00:01\n",
            "   --------------------------------------  203.1/203.1 MB 36.4 MB/s eta 0:00:01\n",
            "   --------------------------------------  203.1/203.1 MB 36.4 MB/s eta 0:00:01\n",
            "   --------------------------------------  203.1/203.1 MB 36.4 MB/s eta 0:00:01\n",
            "   --------------------------------------  203.1/203.1 MB 36.4 MB/s eta 0:00:01\n",
            "   --------------------------------------- 203.1/203.1 MB 13.1 MB/s eta 0:00:00\n",
            "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
            "   ------------ --------------------------- 1.9/6.2 MB 60.1 MB/s eta 0:00:01\n",
            "   ----------------------- ---------------- 3.7/6.2 MB 46.7 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 5.5/6.2 MB 44.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------  6.2/6.2 MB 39.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 6.2/6.2 MB 33.0 MB/s eta 0:00:00\n",
            "Downloading scikit_learn-1.5.2-cp312-cp312-win_amd64.whl (11.0 MB)\n",
            "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
            "   ---- ----------------------------------- 1.3/11.0 MB 40.6 MB/s eta 0:00:01\n",
            "   -------- ------------------------------- 2.3/11.0 MB 29.4 MB/s eta 0:00:01\n",
            "   ------------ --------------------------- 3.4/11.0 MB 27.3 MB/s eta 0:00:01\n",
            "   ------------------ --------------------- 5.2/11.0 MB 30.1 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 6.9/11.0 MB 31.4 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 8.6/11.0 MB 32.3 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 9.8/11.0 MB 31.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------  11.0/11.0 MB 31.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 11.0/11.0 MB 28.5 MB/s eta 0:00:00\n",
            "Downloading onnx-1.17.0-cp312-cp312-win_amd64.whl (14.5 MB)\n",
            "   ---------------------------------------- 0.0/14.5 MB ? eta -:--:--\n",
            "   ---- ----------------------------------- 1.5/14.5 MB 32.9 MB/s eta 0:00:01\n",
            "   -------- ------------------------------- 3.1/14.5 MB 33.3 MB/s eta 0:00:01\n",
            "   ------------- -------------------------- 4.9/14.5 MB 35.3 MB/s eta 0:00:01\n",
            "   ------------------ --------------------- 6.7/14.5 MB 35.9 MB/s eta 0:00:01\n",
            "   ----------------------- ---------------- 8.4/14.5 MB 36.1 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 10.1/14.5 MB 36.0 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 11.8/14.5 MB 36.3 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 13.4/14.5 MB 38.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------  14.5/14.5 MB 36.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 14.5/14.5 MB 32.8 MB/s eta 0:00:00\n",
            "Downloading onnxruntime-1.19.2-cp312-cp312-win_amd64.whl (11.1 MB)\n",
            "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
            "   ----- ---------------------------------- 1.5/11.1 MB 47.6 MB/s eta 0:00:01\n",
            "   --------- ------------------------------ 2.7/11.1 MB 34.5 MB/s eta 0:00:01\n",
            "   --------------- ------------------------ 4.2/11.1 MB 33.3 MB/s eta 0:00:01\n",
            "   --------------------- ------------------ 6.0/11.1 MB 34.8 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 7.2/11.1 MB 32.9 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 8.8/11.1 MB 33.3 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 10.5/11.1 MB 32.8 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 11.1/11.1 MB 29.7 MB/s eta 0:00:00\n",
            "Downloading huggingface_hub-0.26.1-py3-none-any.whl (447 kB)\n",
            "   ---------------------------------------- 0.0/447.4 kB ? eta -:--:--\n",
            "   --------------------------------------- 447.4/447.4 kB 29.1 MB/s eta 0:00:00\n",
            "Downloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
            "   ---------------------------------------- 0.0/179.6 kB ? eta -:--:--\n",
            "   --------------------------------------- 179.6/179.6 kB 10.6 MB/s eta 0:00:00\n",
            "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "   ---------------------------------------- 0.0/301.8 kB ? eta -:--:--\n",
            "   --------------------------------------- 301.8/301.8 kB 18.2 MB/s eta 0:00:00\n",
            "Downloading protobuf-5.28.3-cp310-abi3-win_amd64.whl (431 kB)\n",
            "   ---------------------------------------- 0.0/431.5 kB ? eta -:--:--\n",
            "   --------------------------------------- 431.5/431.5 kB 26.3 MB/s eta 0:00:00\n",
            "Downloading safetensors-0.4.5-cp312-none-win_amd64.whl (286 kB)\n",
            "   ---------------------------------------- 0.0/286.3 kB ? eta -:--:--\n",
            "   --------------------------------------- 286.3/286.3 kB 18.4 MB/s eta 0:00:00\n",
            "Downloading scipy-1.14.1-cp312-cp312-win_amd64.whl (44.5 MB)\n",
            "   ---------------------------------------- 0.0/44.5 MB ? eta -:--:--\n",
            "   - -------------------------------------- 1.2/44.5 MB 37.7 MB/s eta 0:00:02\n",
            "   -- ------------------------------------- 2.6/44.5 MB 33.5 MB/s eta 0:00:02\n",
            "   ---- ----------------------------------- 4.6/44.5 MB 36.6 MB/s eta 0:00:02\n",
            "   ----- ---------------------------------- 6.2/44.5 MB 36.2 MB/s eta 0:00:02\n",
            "   ------ --------------------------------- 7.6/44.5 MB 34.5 MB/s eta 0:00:02\n",
            "   ------- -------------------------------- 8.7/44.5 MB 32.8 MB/s eta 0:00:02\n",
            "   -------- ------------------------------- 9.9/44.5 MB 31.8 MB/s eta 0:00:02\n",
            "   --------- ------------------------------ 11.1/44.5 MB 29.8 MB/s eta 0:00:02\n",
            "   ----------- ---------------------------- 12.7/44.5 MB 31.2 MB/s eta 0:00:02\n",
            "   ------------ --------------------------- 14.3/44.5 MB 31.2 MB/s eta 0:00:01\n",
            "   -------------- ------------------------- 16.0/44.5 MB 29.7 MB/s eta 0:00:01\n",
            "   --------------- ------------------------ 17.6/44.5 MB 31.2 MB/s eta 0:00:01\n",
            "   ----------------- ---------------------- 19.3/44.5 MB 32.7 MB/s eta 0:00:01\n",
            "   ------------------ --------------------- 21.0/44.5 MB 34.4 MB/s eta 0:00:01\n",
            "   -------------------- ------------------- 22.5/44.5 MB 34.6 MB/s eta 0:00:01\n",
            "   --------------------- ------------------ 24.0/44.5 MB 34.4 MB/s eta 0:00:01\n",
            "   ----------------------- ---------------- 25.9/44.5 MB 34.4 MB/s eta 0:00:01\n",
            "   ------------------------ --------------- 27.5/44.5 MB 34.4 MB/s eta 0:00:01\n",
            "   -------------------------- ------------- 29.3/44.5 MB 36.4 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 30.9/44.5 MB 36.4 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 32.5/44.5 MB 34.4 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 34.2/44.5 MB 34.4 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 35.7/44.5 MB 34.6 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 37.3/44.5 MB 36.3 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 38.9/44.5 MB 34.4 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 40.5/44.5 MB 34.4 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 42.0/44.5 MB 34.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------  43.9/44.5 MB 34.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------  44.5/44.5 MB 34.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------  44.5/44.5 MB 34.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 44.5/44.5 MB 26.2 MB/s eta 0:00:00\n",
            "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
            "Downloading tokenizers-0.20.1-cp312-none-win_amd64.whl (2.4 MB)\n",
            "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
            "   -------------------------- ------------- 1.5/2.4 MB 48.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 2.4/2.4 MB 30.0 MB/s eta 0:00:00\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "   ---------------------------------------- 0.0/46.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 46.0/46.0 kB 2.2 MB/s eta 0:00:00\n",
            "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
            "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
            "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
            "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
            "   ------------------------------------- -- 1.6/1.7 MB 49.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 1.7/1.7 MB 27.2 MB/s eta 0:00:00\n",
            "Downloading setuptools-75.2.0-py3-none-any.whl (1.2 MB)\n",
            "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
            "   ---------------------------------------  1.2/1.2 MB 77.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 1.2/1.2 MB 38.7 MB/s eta 0:00:00\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "   ---------------------------------------- 0.0/86.8 kB ? eta -:--:--\n",
            "   ---------------------------------------- 86.8/86.8 kB 4.8 MB/s eta 0:00:00\n",
            "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
            "   --------------------------------------- 536.2/536.2 kB 17.0 MB/s eta 0:00:00\n",
            "Downloading pyreadline3-3.5.4-py3-none-any.whl (83 kB)\n",
            "   ---------------------------------------- 0.0/83.2 kB ? eta -:--:--\n",
            "   ---------------------------------------- 83.2/83.2 kB 4.6 MB/s eta 0:00:00\n",
            "Installing collected packages: mpmath, flatbuffers, threadpoolctl, sympy, setuptools, scipy, safetensors, pyreadline3, protobuf, networkx, joblib, fsspec, filelock, torch, scikit-learn, onnx, humanfriendly, huggingface-hub, tokenizers, coloredlogs, transformers, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 filelock-3.16.1 flatbuffers-24.3.25 fsspec-2024.10.0 huggingface-hub-0.26.1 humanfriendly-10.0 joblib-1.4.2 mpmath-1.3.0 networkx-3.4.2 onnx-1.17.0 onnxruntime-1.19.2 protobuf-5.28.3 pyreadline3-3.5.4 safetensors-0.4.5 scikit-learn-1.5.2 scipy-1.14.1 setuptools-75.2.0 sympy-1.13.1 threadpoolctl-3.5.0 tokenizers-0.20.1 torch-2.5.0 transformers-4.46.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 24.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "c:\\Users\\arhaa\\Desktop\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spam\n",
            "0    4825\n",
            "1     747\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\arhaa\\Desktop\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\arhaa\\.cache\\huggingface\\hub\\models--prajjwal1--bert-tiny. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "c:\\Users\\arhaa\\Desktop\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 - Training Loss: 0.6314\n",
            "Epoch 2 - Training Loss: 0.5662\n",
            "Epoch 3 - Training Loss: 0.5309\n",
            "Validation Accuracy: 0.9258\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         Ham       0.91      0.95      0.93      1216\n",
            "        Spam       0.94      0.91      0.92      1197\n",
            "\n",
            "    accuracy                           0.93      2413\n",
            "   macro avg       0.93      0.93      0.93      2413\n",
            "weighted avg       0.93      0.93      0.93      2413\n",
            "\n",
            "Email: \"Congratulations! You've won a $1,000 Walmart gift card. Click here to claim your prize!\" - Prediction: Spam\n",
            "Email: \"Can we schedule a meeting for next week?\" - Prediction: Ham\n",
            "Email: \"Hurry!!! Halloween is Near Get a Halloween Discount Today USE PROMO CODE 'halloween 2023' \" - Prediction: Spam\n",
            "Email: \"Hey, just checking in to see how you're doing.\" - Prediction: Ham\n",
            "Model exported to ONNX format at: bert_spam_detection.onnx\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers torch scikit-learn onnx onnxruntime\n",
        "\n",
        "# Import necessary libraries\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "import onnx\n",
        "\n",
        "# Load the spam dataset\n",
        "data = pd.read_csv('spam.csv', encoding='ISO-8859-1')\n",
        "data.columns = data.columns.str.strip()  # Strip whitespace from column names\n",
        "data.rename(columns={'v1': 'Category', 'v2': 'Message'}, inplace=True)\n",
        "data['Spam'] = data['Category'].apply(lambda x: 1 if x == 'spam' else 0)\n",
        "\n",
        "# Check for class distribution\n",
        "print(data['Spam'].value_counts())\n",
        "\n",
        "# Address class imbalance if present\n",
        "ham_samples = data[data['Spam'] == 0]\n",
        "spam_samples = data[data['Spam'] == 1]\n",
        "\n",
        "if len(ham_samples) > len(spam_samples):\n",
        "    spam_samples = spam_samples.sample(len(ham_samples), replace=True)  # Oversample spam\n",
        "data = pd.concat([ham_samples, spam_samples]).sample(frac=1, random_state=42)  # Shuffle the dataset\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    data['Message'], data['Spam'], test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "# Define the custom dataset class for tokenization\n",
        "class SpamDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):  # Corrected __init__\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):  # Corrected __len__\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):  # Corrected __getitem__\n",
        "        text = str(self.texts.iloc[idx])\n",
        "        label = self.labels.iloc[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Initialize BertTiny tokenizer and model for sequence classification\n",
        "model_name = \"prajjwal1/bert-tiny\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=2  # Binary classification for spam detection\n",
        ")\n",
        "\n",
        "# Freeze initial layers to fine-tune the classifier head\n",
        "def freeze_bert_layers(model, num_layers_to_freeze=2):\n",
        "    for param in model.bert.embeddings.parameters():\n",
        "        param.requires_grad = False\n",
        "    for layer in model.bert.encoder.layer[:num_layers_to_freeze]:\n",
        "        for param in layer.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "freeze_bert_layers(model)\n",
        "\n",
        "# Prepare the data loaders\n",
        "train_dataset = SpamDataset(train_texts, train_labels, tokenizer)\n",
        "val_dataset = SpamDataset(val_texts, val_labels, tokenizer)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "# Set up the device, optimizer, and scheduler\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5)\n",
        "total_steps = len(train_loader) * 3  # Assuming 3 epochs for training\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "# Training loop\n",
        "def train_model(model, train_loader, val_loader, device, num_epochs=3):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch + 1} - Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "# Start the training process\n",
        "train_model(model, train_loader, val_loader, device, num_epochs=3)\n",
        "\n",
        "# Testing and Accuracy Evaluation\n",
        "def evaluate_model(model, val_loader, device):\n",
        "    model.eval()\n",
        "    val_preds = []\n",
        "    val_labels = []\n",
        "    total_val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            total_val_loss += outputs.loss.item()\n",
        "\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "            val_preds.extend(preds.cpu().numpy())\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(val_labels, val_preds)\n",
        "    print(f'Validation Accuracy: {accuracy:.4f}')\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(val_labels, val_preds, target_names=['Ham', 'Spam']))\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "evaluate_model(model, val_loader, device)\n",
        "\n",
        "# Test the model on new examples\n",
        "def test_model(model, tokenizer, texts, device):\n",
        "    model.eval()\n",
        "    inputs = tokenizer(\n",
        "        texts,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.argmax(outputs.logits, dim=1)\n",
        "        return predictions.cpu().numpy()\n",
        "\n",
        "# Example emails for testing\n",
        "test_emails = [\n",
        "    \"Congratulations! You've won a $1,000 Walmart gift card. Click here to claim your prize!\",\n",
        "    \"Can we schedule a meeting for next week?\",\n",
        "    \"Hurry!!! Halloween is Near Get a Halloween Discount Today USE PROMO CODE 'halloween 2023' \",\n",
        "    \"Hey, just checking in to see how you're doing.\"\n",
        "]\n",
        "\n",
        "# Get predictions for the test emails\n",
        "predictions = test_model(model, tokenizer, test_emails, device)\n",
        "\n",
        "# Display predictions\n",
        "for email, pred in zip(test_emails, predictions):\n",
        "    label = 'Spam' if pred == 1 else 'Ham'\n",
        "    print(f\"Email: \\\"{email}\\\" - Prediction: {label}\")\n",
        "\n",
        "# Save the model in ONNX format\n",
        "onnx_file_path = \"bert_spam_detection.onnx\"\n",
        "dummy_input = torch.ones(1, 128, dtype=torch.int64).to(device)\n",
        "dummy_attention_mask = torch.ones(1, 128, dtype=torch.int64).to(device)\n",
        "\n",
        "# Export the model to ONNX with opset version 14\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    (dummy_input, dummy_attention_mask),\n",
        "    onnx_file_path,\n",
        "    input_names=['input_ids', 'attention_mask'],\n",
        "    output_names=['output'],\n",
        "    dynamic_axes={'input_ids': {0: 'batch_size'}, 'attention_mask': {0: 'batch_size'}, 'output': {0: 'batch_size'}},\n",
        "    opset_version=14\n",
        ")\n",
        "\n",
        "print(f\"Model exported to ONNX format at: {onnx_file_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
